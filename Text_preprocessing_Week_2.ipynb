{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_preprocessing_Week_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I2s3F7Lp6E1PmCvJAAfNvyAD1ykMWVFN",
      "authorship_tag": "ABX9TyNFoY0xSiK19DrVnSIcH1kP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DODO-YONG/Study/blob/main/Text_preprocessing_Week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 모듈 Import\n"
      ],
      "metadata": {
        "id": "YNjW1D0aRZ9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NJ_H_Y2QdJp"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import konlpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_profiling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/공부/풀입스쿨19_NLP'"
      ],
      "metadata": {
        "id": "LHO7GEF7SGBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zif_bEPRDcn",
        "outputId": "d923362d-1b6a-4b5c-b670-ca7e61f4ad5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path + \"/data/spam.csv\", encoding='latin1')"
      ],
      "metadata": {
        "id": "p5ZS5EJPQyhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dhZdVqc4RB7S",
        "outputId": "fec8d475-f02b-40e8-e039-8429c8eea4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        v1                                                 v2 Unnamed: 2  \\\n",
              "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
              "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
              "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
              "...    ...                                                ...        ...   \n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
              "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
              "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
              "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
              "5571   ham                         Rofl. Its true to its name        NaN   \n",
              "\n",
              "     Unnamed: 3 Unnamed: 4  \n",
              "0           NaN        NaN  \n",
              "1           NaN        NaN  \n",
              "2           NaN        NaN  \n",
              "3           NaN        NaN  \n",
              "4           NaN        NaN  \n",
              "...         ...        ...  \n",
              "5567        NaN        NaN  \n",
              "5568        NaN        NaN  \n",
              "5569        NaN        NaN  \n",
              "5570        NaN        NaN  \n",
              "5571        NaN        NaN  \n",
              "\n",
              "[5572 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b299c7f-2c5d-4f66-a85f-bc14d8f82fe7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b299c7f-2c5d-4f66-a85f-bc14d8f82fe7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b299c7f-2c5d-4f66-a85f-bc14d8f82fe7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b299c7f-2c5d-4f66-a85f-bc14d8f82fe7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리\n",
        "- 자연어 전처리는 크게 토큰화, 정제, 정규화"
      ],
      "metadata": {
        "id": "Mn1B5rY6r2WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화\n",
        "- 토큰의 기준을 단어로 하는경우 단어의 토큰화\n",
        "- 다만 단어는 단어외 단어구, 의미를 갖는 문자열로도 간주\n",
        "- 가장 간단하게는 구두점(.,?!등) 과 띄어쓰기를 기준으로 구분"
      ],
      "metadata": {
        "id": "aBV7gXQVTRYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단순하게 띄어쓰기나 ' , \" ; : 등으로 하게되면 상황에 따라서 문제가 발생한다.\n",
        "- 예시) Ph.D, $45.55 01/02/21\n",
        "- rock n roll\n"
      ],
      "metadata": {
        "id": "4VRvb-RRT1Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- word_tokenize와 WordPunctTokenizer를 사용해서 아포스트로피를 어떻게 처리하는지 확인"
      ],
      "metadata": {
        "id": "DugUs6EGw2vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "cYyQ3IbBSiYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKlGXhaSUJBv",
        "outputId": "c825b286-7891-43db-acb9-afef4341dd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZU5-wAZT_BK",
        "outputId": "cf5dfeab-d076-4d21-ba57-f0bf2cca63bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEf-XgS1UE99",
        "outputId": "9fc0a9a7-87d1-4d43-8351-f56ea66b169d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 표준 토큰화 예제\n"
      ],
      "metadata": {
        "id": "pFBYZMrfVNWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_zDm4ReUmKJ",
        "outputId": "30619370-116c-40dd-e408-090443abe8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문장 토큰화\n",
        "- .이나 !?등으로 구분안됨"
      ],
      "metadata": {
        "id": "pUhgR9VdVfqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print('문장 토큰화1 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAAr67AxVRUU",
        "outputId": "a386c767-47f1-41b7-9dfe-133b1fa42963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print('문장 토큰화2 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrHPfP3BWQZW",
        "outputId": "5da799c2-84e4-416c-bb0c-8fde7bbcd794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장 토큰화 한국어의 경우"
      ],
      "metadata": {
        "id": "FeoYZqAvWsvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "id": "m-h-xexEWoSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1uvxghxWxTh",
        "outputId": "0d9004ba-4af6-4bb4-a749-ddb803b15285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어에서의 토큰화의 어려움"
      ],
      "metadata": {
        "id": "35-G4WgiXFxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어는 띄어쓰기 토큰화가 잘통하지만 한글은 아니다.\n",
        "- 한국어의 경우 띄어쓰기가 되는 단위를 '어절'이라 칭한다.\n",
        "- 한국어NLP에서는 이런 어절 토큰화를 지양한다.\n",
        "- 이유는 한국어는 영어와 다른 형태인 교착어라는 점이다.\n",
        "- 교착어란 조사 어미등을 붙여서 말을 만드는 언어이다."
      ],
      "metadata": {
        "id": "vLN5l4hUXJzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. 교착어의 특성\n",
        "- 조사\n",
        "ex) 그가, 그에게, 그를, 그와 등등 그 뒤에띄어쓰기 없이 조사가 붙는다. 한국어 NLP에서는 이를 분리해줄 필요가 있다.\n",
        "\n",
        "- 형태소\n",
        "    - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소(그 자체로 단어가 됨)\n",
        "    - 의존 형태소 : 다른 형태소와 사용되는 형태소\n",
        "- 예를 들어 다음과 같은 문장이 있다고 합시다.\n",
        "\n",
        "문장 : 에디가 책을 읽었다\n",
        "이 문장을 띄어쓰기 단위 토큰화를 수행한다면 다음과 같은 결과를 얻습니다.\n",
        "\n",
        "['에디가', '책을', '읽었다']\n",
        "하지만 이를 형태소 단위로 분해하면 다음과 같습니다.\n",
        "\n",
        "자립 형태소 : 에디, 책\n",
        "의존 형태소 : -가, -을, 읽-, -었, -다\n",
        "\n",
        "## 2.한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
        "- EX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
        "\n",
        "- EX2) Tobeornottobethatisthequestion"
      ],
      "metadata": {
        "id": "_Zs0bNITXsTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 품사 태깅\n",
        "- ex) fly : V(날다), n(파리)\n",
        "- 상황에 따라 같은 단어도 품사가 다르다.\n",
        "- 해당 단어가 어떤 품사로 쓰였는것이 주요 지표가 될수 있다.\n",
        "- 이를 구분해놓는작업을 품사태깅 POS(Part-of-speech tagging)라 한다."
      ],
      "metadata": {
        "id": "uTnf84Zpn5j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅합니다.\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print('단어 토큰화 :',tokenized_sentence)\n",
        "print('품사 태깅 :',pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxd9tzPOl5mi",
        "outputId": "a60067f6-e1fa-4282-a25f-ca5cb4e6bf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 NLP에서 형태소 분석기를 사용하여 형태소 토큰화 수행(Okt, 꼬꼬마)\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qivqDstpfHl",
        "outputId": "817e460e-80ce-46e2-c847-7e6709c38838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycsaf1ABp1_5",
        "outputId": "b6095979-6248-4ec3-d49a-e1dbf1c2411a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소마다 결과가 다르게 나온다"
      ],
      "metadata": {
        "id": "BpNxJNHtqI6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정제와 정규화\n",
        "- 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화\n",
        "- 토큰화 작업 전, 후에 텍스트 데이터를 용도에 맞게 정규 및 정규화 함\n",
        "    - 정제 : 코퍼스로 부터 노이즈 데이터 제거\n",
        "    - 정규화 : 표현방법이 다른 단어들 통합시켜 같은 단어로 만들어준다.(표제어 추출, 어간 추출)\n",
        "- 정제는 완벽하기 어려워 지속적으로 이루어지기도 한다."
      ],
      "metadata": {
        "id": "Y-im_4Qqqex8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 대소문자 통합, 등장빈도가 작거나 길이가 짧은 단어를 제거\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTigCxwSqbW0",
        "outputId": "95665a1e-628f-4c05-e519-cc11a5ac5f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규표현식으로 노이즈 제거\n",
        "# ex) 크롤링 시 html 태그\n"
      ],
      "metadata": {
        "id": "60Tzk0NZtAV3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
        "- 기능 : 보기에는 다른 단어지만 하나의 단어로 일반화 시킬 수 있다면 묶어서 단어의 수를 줄이겠다."
      ],
      "metadata": {
        "id": "EkpBjCHHtfjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 표제어 추출\n",
        "- ex) am, are, is == be\n",
        "\n",
        "- 형태소 : 의미를 가진 가장 작은 단위\n",
        "    - 어간 : 단어의 의미를 담고 있는 핵심 부분\n",
        "    - 접사 : 단어에 추가적인 의미를 주는 부분\n",
        "    - ex) cats : cat(어간), s(접사)"
      ],
      "metadata": {
        "id": "4mYsXzClt7i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "jqczH8A1up4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 :',words)\n",
        "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY6QDut9te3t",
        "outputId": "8e430649-3ef8-483b-cb79-50c80d020319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dy나 ha같은 의미를 알수 없는 단어 출력\n",
        "# 본래 단어의 품사 정보를 알아야 정확한 결과를 얻을 수 있다."
      ],
      "metadata": {
        "id": "s9Ec3g_Bulwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('dies', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "snnIeimJyRYu",
        "outputId": "0fb9bca2-7fb9-45f5-b48a-919ad4a2476b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('watched', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qRfDpgP-yd0j",
        "outputId": "839acbeb-8858-4363-9582-702716bafd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'watch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('has', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lPYwtTm8yfes",
        "outputId": "8fe0efa7-99a2-42b0-8bd5-416d347cabe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어간 추출(Stemming)\n",
        "- 정해진 규칙을 보고 단어의 어미를 자르는 어림짐작의 작업\n",
        "- 정확하지 않다. 사전에 존재하지 않는 단어일 수도 있다."
      ],
      "metadata": {
        "id": "jGTVMTumypaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print('어간 추출 전 :', tokenized_sentence)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGg8a3myj3g",
        "outputId": "76361071-022f-4b8d-e001-982de2ca614b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "\n",
        "print('어간 추출 전 :',words)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVpE7DECzGRJ",
        "outputId": "ff842358-0e8e-48e4-e5de-4631ac9f5201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
            "어간 추출 후 : ['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리시 준수한 선택이다.\n",
        "# 두개의 어간 추출기 결과를 비교해보자.\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('어간 추출 전 :', words)\n",
        "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
        "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7monrbb1zpQ6",
        "outputId": "b1c8df2e-1b0b-4e3c-b865-c07fff1cd67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 표제어(Lemmatization) 추출과 어간(Stemming) 추출의 비교\n",
        "- Stemming\n",
        "    - am → am\n",
        "    - the going → the go\n",
        "    - having → hav\n",
        "\n",
        "- Lemmatization\n",
        "    - am → be\n",
        "    - the going → the going\n",
        "    - having → have"
      ],
      "metadata": {
        "id": "Dp4zxSmF0Mmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어에서의 어간 추출 생략"
      ],
      "metadata": {
        "id": "-I4tZiSO00Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 불용어\n",
        "- 자주 등장하지만 큰의미가 없는 단어는 토큰화에 큰 의미가 없다.\n",
        "- 따라서 제거한다\n",
        "- ex) I, my, me, 조사 접미사"
      ],
      "metadata": {
        "id": "RgOZ7XyA2scj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK에서 불용어 확인하기"
      ],
      "metadata": {
        "id": "cnweiYKtSMOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "W8V902Mm2sMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv1f7XzGSaNC",
        "outputId": "649bb71f-6c8c-4106-8052-90ec4b7eb2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JLhh_1i0BFX",
        "outputId": "66779e84-96ee-4e9b-bf9c-674efb3bfd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vkZFpOfSm4D",
        "outputId": "f266d28e-b9a4-4c30-b70e-972b4e10509e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "    if word not in stop_words: \n",
        "        result.append(word) \n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANwUejX3SY4U",
        "outputId": "ebf99713-9232-4376-e217-f0afeea855d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#'is', 'not', 'an'과 같은 단어들이 문장에서 제거"
      ],
      "metadata": {
        "id": "YSh_bcChSlhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한국어에서 불용어 제거하기\n",
        "- 임의로 불용어를 지정하여 제거"
      ],
      "metadata": {
        "id": "JZRfNlvKSyAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7twgO6ySu7j",
        "outputId": "25243f98-1124-4dfe-8f0a-a6581141587a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 원-핫 인코딩\n",
        "- 자연어 처리에서 문자를 숫자로 바꾸는 기법들 중 하나\n",
        "- 단어의 집합 : 텍스트의 모든 단어의 중복을 제거한 상태로 모아둔것\n",
        "\n",
        "## 원핫 인코딩이란?\n",
        "- 단어 집합의 크기를 벡터의 차원으로 하고 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른인덱스에 0을 부여하는 단어의 벡터 표현방식\n",
        "- 다른말로 원핫 벡터"
      ],
      "metadata": {
        "id": "1i9pTdCaVBP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 먼저 각 단어에 고유한 정수를 부여"
      ],
      "metadata": {
        "id": "admKKGS8WcZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt  \n",
        "\n",
        "okt = Okt()  \n",
        "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZQYRsDS8_k",
        "outputId": "29f5e86c-08cb-47fb-b227-44bc344fe41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
        "print('단어 집합 :',word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smKsj6xvWSpq",
        "outputId": "8c8fdd6a-d7f4-4bbe-98b5-8b08fdea1aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당위치에 1을 부여"
      ],
      "metadata": {
        "id": "jAQnsPmfWjJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector = [0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "lwBdEY-aWUgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFcUtDqcWYVX",
        "outputId": "6db5d64a-9367-4268-d1e4-6b84b8a4a7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 원핫 인코딩의 한계\n",
        "1. 단어가 늘어날 수록 벡터를 저장하기 위한 공간이 늘어난다.\n",
        "    - 다른 표현으로 벡터의 차원이 늘어난다.\n",
        "2. 단어의 유사도를 표현하지 못한다\n",
        "    - 이를 해결하기 위한 방법은\n",
        "    1. 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HA\n",
        "    2. 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다."
      ],
      "metadata": {
        "id": "rb3E2J7bW6H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "8nw68l4NWaS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}